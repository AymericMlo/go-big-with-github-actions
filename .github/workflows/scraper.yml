name: First Scraper

on:
  workflow_dispatch:
    inputs: #On va ajouter des possibilités de déclencher le scrape manuellement
      state: #On va donner la possibilité de scraper pour un Etat donné. 
        description: 'U.S. state to scrape'
        required: true
        default: 'ia' #Si pas d'input, on scrape les données pour l'Ohio. 
  schedule:
  - cron: "0 0 * * *" #Les actions seront lancées tous les jours à 00:00 UTC

permissions: #On doit donner des permissions au job
  contents: write 

jobs:
  scrape:
    name: Scrape
    runs-on: ubuntu-latest
    steps:
      - name: Checkout #Cela clone notre repository sur le serveur pour que toutes les étapes suivantes puissent y accéder. Nécessaire pour pouvoir sauvegarder les données scraper dans le repo à la fin du workflow. 
        uses: actions/checkout@v4 

      - name: Install Python #On installe Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install scraper #On installe le scraper, qui se trouve dans un package Python dédié, dont la documentation est ici : https://warn-scraper.readthedocs.io/en/latest/usage.html
        run: pip install warn-scraper
         
      - name: Scrape
        run: warn-scraper ${{ inputs.state }} --data-dir ./data/ #On scrape l'Etat indiqué dans l'input utilisateur, plus haut. Et on stocke les résultats dans le dossier ./data/, à la racine du repo.

      - name: Save datestamp #En cas d'inactivité prolongée, GitHub désactive automatiquement les workflows. Pour prévenir ça, on commit et update un fichier texte à chaque fois que l'action tourne. 
        run: date > ./data/latest-scrape.txt
        
      - name: Commit and push #On va commit les données scrappées et les pousser dans GitHub. 
        run:  |
          git config user.name "GitHub Actions"
          git config user.email "actions@users.noreply.github.com"
          git add ./data/
          git commit -m "Latest data for ${{ inputs.state }}" && git push || true
       
  
